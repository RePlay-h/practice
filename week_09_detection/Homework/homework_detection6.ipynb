{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "8a7f379a-833c-4cfe-8860-3653c6fb0783",
      "metadata": {
        "id": "8a7f379a-833c-4cfe-8860-3653c6fb0783"
      },
      "source": [
        "<h3 style=\"text-align: center;\"><b>Школа глубокого обучения ФПМИ МФТИ</b></h3>\n",
        "\n",
        "<h3 style=\"text-align: center;\"><b>Домашнее задание. Детекция объектов</b></h3>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "149f6062-7f0c-42b9-ae36-0f360c852d25",
      "metadata": {
        "id": "149f6062-7f0c-42b9-ae36-0f360c852d25"
      },
      "source": [
        "В этом домашнем задании мы продолжим работу над детектором из семинара, поэтому при необходимости можете заимствовать оттуда любой код.\n",
        "\n",
        "Домашнее задание можно разделить на следующие части:\n",
        "\n",
        "* Переделываем модель [4]\n",
        "  * Backbone[1],\n",
        "  * Neck [2],\n",
        "  * Head [1]\n",
        "* Label assignment [3]:\n",
        "  * TAL [3]\n",
        "* Лоссы [1]:\n",
        "  * CIoU loss [1]\n",
        "* Кто больше? [5]\n",
        "  * 0.05 mAP [1]\n",
        "  * 0.1 mAP  [2]\n",
        "  * 0.2 mAP [5]\n",
        "\n",
        "**Максимальный балл:** 10 баллов. (+3 балла бонус)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchmetrics"
      ],
      "metadata": {
        "id": "IIT__7jHqn0k",
        "outputId": "0bb1525a-86ec-446e-9b68-48a94d0c755b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "IIT__7jHqn0k",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-1.8.2-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.12/dist-packages (from torchmetrics) (2.0.2)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.12/dist-packages (from torchmetrics) (25.0)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from torchmetrics) (2.9.0+cu126)\n",
            "Collecting lightning-utilities>=0.8.0 (from torchmetrics)\n",
            "  Downloading lightning_utilities-0.15.2-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (75.2.0)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.12/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.15.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.20.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.5.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->torchmetrics) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->torchmetrics) (3.0.3)\n",
            "Downloading torchmetrics-1.8.2-py3-none-any.whl (983 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m983.2/983.2 kB\u001b[0m \u001b[31m58.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.15.2-py3-none-any.whl (29 kB)\n",
            "Installing collected packages: lightning-utilities, torchmetrics\n",
            "Successfully installed lightning-utilities-0.15.2 torchmetrics-1.8.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "6fb023c6-434e-47b4-b048-73683a6ce482",
      "metadata": {
        "id": "6fb023c6-434e-47b4-b048-73683a6ce482"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import albumentations as A\n",
        "\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset\n",
        "from albumentations.pytorch.transforms import ToTensorV2\n",
        "\n",
        "import math\n",
        "from functools import partial\n",
        "from collections import Counter, defaultdict\n",
        "\n",
        "import io\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import timm\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "from torchvision.ops import nms, box_iou\n",
        "from torchvision.models.detection.anchor_utils import AnchorGenerator\n",
        "from torchvision.ops import FeaturePyramidNetwork\n",
        "\n",
        "from torchmetrics.detection import MeanAveragePrecision\n",
        "from typing import Dict, List"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "42e2b5b2-113e-43d5-a134-31b51a256671",
      "metadata": {
        "id": "42e2b5b2-113e-43d5-a134-31b51a256671"
      },
      "source": [
        "### Загрузка данных\n",
        "\n",
        "Мы продолжаем работу с датасетом из семинара - Halo infinite ([сслыка](https://universe.roboflow.com/graham-doerksen/halo-infinite-angel-aim)). Загрузка данных и создание датасета полностью скопированы из семинара.\n",
        "\n",
        "Сначала загружаем данные"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "b2eecb64-16a2-4b97-b627-890ea316a594",
      "metadata": {
        "id": "b2eecb64-16a2-4b97-b627-890ea316a594",
        "outputId": "d4c99259-25b7-42a6-fbd0-de8de1311f3a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'halo-infinite-angel-videogame'...\n",
            "remote: Enumerating objects: 30, done.\u001b[K\n",
            "remote: Total 30 (delta 0), reused 0 (delta 0), pack-reused 30 (from 1)\u001b[K\n",
            "Unpacking objects: 100% (30/30), 6.28 KiB | 918.00 KiB/s, done.\n"
          ]
        }
      ],
      "source": [
        "! git clone https://huggingface.co/datasets/Francesco/halo-infinite-angel-videogame\n",
        "\n",
        "splits = {'train': 'data/train-00000-of-00001-0d6632d599c29801.parquet',\n",
        "          'validation': 'data/validation-00000-of-00001-c6b77a557eeedd52.parquet',\n",
        "          'test': 'data/test-00000-of-00001-866d29d8989ea915.parquet'}\n",
        "\n",
        "df_train = pd.read_parquet(\"/content/halo-infinite-angel-videogame/\" + splits[\"train\"])\n",
        "df_test = pd.read_parquet(\"/content/halo-infinite-angel-videogame/\" + splits[\"test\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ade755e-4471-4c79-84bc-8b560484e833",
      "metadata": {
        "id": "1ade755e-4471-4c79-84bc-8b560484e833"
      },
      "source": [
        "Создаем датасет для предобработки данных"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "5a2f8ba9-5ae2-4d33-9d9d-5a0f0bb36d38",
      "metadata": {
        "id": "5a2f8ba9-5ae2-4d33-9d9d-5a0f0bb36d38"
      },
      "outputs": [],
      "source": [
        "class HaloDataset(Dataset):\n",
        "    def __init__(self, dataframe, transform=None):\n",
        "        df_objects = pd.json_normalize(dataframe['objects'])[[\"bbox\", \"category\"]]\n",
        "        df_images = pd.json_normalize(dataframe['image'])[[\"bytes\"]]\n",
        "        self.data = dataframe[[\"image_id\"]].join(df_objects).join(df_images)\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"Загружаем данные и разметку для объекта с индексом `idx`.\n",
        "\n",
        "        labels: List[int] Набор классов для каждого ббокса,\n",
        "        boxes: List[List[int]] Набор ббоксов в формате (x_min, y_min, w, h).\n",
        "        \"\"\"\n",
        "        row = self.data.iloc[idx]\n",
        "        image = Image.open(io.BytesIO(row[\"bytes\"]))\n",
        "        image = np.array(image)\n",
        "\n",
        "        target = {}\n",
        "        target[\"image_id\"] = row[\"image_id\"]\n",
        "\n",
        "        labels = [row[\"category\"]] if isinstance(row[\"category\"], int) else row['category']\n",
        "        # Вычитаем единицу чтобы классы начинались с нуля\n",
        "        labels = [label - 1 for label in labels]\n",
        "        boxes = row['bbox'].tolist()\n",
        "\n",
        "        if self.transform is not None:\n",
        "            transformed = self.transform(image=image, bboxes=boxes, labels=labels)\n",
        "            image, boxes, labels = transformed[\"image\"], transformed[\"bboxes\"], transformed[\"labels\"]\n",
        "        else:\n",
        "            image = transforms.ToTensor()(image)\n",
        "\n",
        "        target['boxes'] = torch.tensor(np.array(boxes), dtype=torch.float16)\n",
        "        target['labels'] = torch.tensor(labels, dtype=torch.int64)\n",
        "        return image, target\n",
        "\n",
        "def collate_fn(batch):\n",
        "    batch = tuple(zip(*batch))\n",
        "    images = torch.stack(batch[0])\n",
        "    return images, batch[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e864d642-45eb-4733-a763-2b2b7c711929",
      "metadata": {
        "id": "e864d642-45eb-4733-a763-2b2b7c711929"
      },
      "source": [
        "Чтобы модель не переобучалась, можно добавить больше аугментаций, весь список можно посмотреть тут [[ссылка](https://explore.albumentations.ai/)].\n",
        "\n",
        "Какие можно использовать аугментации?\n",
        "* Добавить зум `RandomResizedCrop`,\n",
        "* Сделать цветовые аугментации типа `RandomBrightnessContrast` и/или `HueSaturationValue`,\n",
        "* Добавить шум `GaussNoise`,\n",
        "* Вырезать случайные части изображения `CoarseDropout`,\n",
        "* И любые другие!\n",
        "\n",
        "Аугментации можно комбинировать посредствам `A.OneOf`, `A.SomeOf` или `A.RandomOrder`.\n",
        "\n",
        "Хоть аугментации ограничиваются только вашей фантазией, перед обучением советуем посмотреть на результат преобразований и убедиться, что изображение ещё поддается детекции:)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "9c8c7095-9b77-4716-86cd-9b3e7dc3890c",
      "metadata": {
        "id": "9c8c7095-9b77-4716-86cd-9b3e7dc3890c"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "\n",
        "mean = (0.485, 0.456, 0.406)\n",
        "std = (0.229, 0.224, 0.225)\n",
        "\n",
        "train_transform = A.Compose(\n",
        "    [\n",
        "        A.Normalize(mean=mean, std=std),\n",
        "        A.Downscale(\n",
        "          scale_range=(0.5, 0.75),\n",
        "          interpolation_pair={'downscale': cv2.INTER_NEAREST, 'upscale': cv2.INTER_LINEAR},\n",
        "          p=0.5\n",
        "        ),\n",
        "        A.GaussianBlur(\n",
        "            sigma_limit=(3.0, 7.0),\n",
        "            blur_limit=0,\n",
        "            p=1.0\n",
        "        ),\n",
        "        A.Sharpen(\n",
        "          alpha=(0.2, 0.5),\n",
        "          lightness=(0.5, 1.0),\n",
        "          method='kernel',\n",
        "          p=1.0\n",
        "        ),\n",
        "        A.ChromaticAberration(\n",
        "          primary_distortion_limit=0.05,\n",
        "          secondary_distortion_limit=0.1,\n",
        "          mode='green_purple',\n",
        "          interpolation=cv2.INTER_LINEAR,\n",
        "          p=1.0\n",
        "        ),\n",
        "        A.CoarseDropout(\n",
        "          num_holes_range=(3, 6),\n",
        "          hole_height_range=(10, 20),\n",
        "          hole_width_range=(10, 20),\n",
        "          fill=\"inpaint_ns\",\n",
        "          p=1.0\n",
        "        ),\n",
        "        A.RandomResizedCrop(\n",
        "          size=(64, 64),\n",
        "          scale=(0.5, 0.9),\n",
        "          ratio=(0.75, 1.33),\n",
        "          interpolation=cv2.INTER_LINEAR,\n",
        "          mask_interpolation=cv2.INTER_NEAREST,\n",
        "          area_for_downscale=\"image\",\n",
        "          p=1.0\n",
        "        ),\n",
        "        A.HorizontalFlip(p=0.5),\n",
        "        A.RandomBrightnessContrast(p=0.2),\n",
        "        A.GaussNoise(std_range=(0.1, 0.2), p=1.0),\n",
        "        A.Solarize(threshold_range=(0.5, 0.5), p=1.0),\n",
        "        ToTensorV2(),\n",
        "    ],\n",
        "    bbox_params=A.BboxParams(format='coco', label_fields=['labels'])\n",
        ")\n",
        "\n",
        "test_transform = A.Compose(\n",
        "    [\n",
        "        A.Normalize(mean=mean, std=std),\n",
        "        ToTensorV2(),\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "02c6edb2-f020-413c-8bf5-6b149e0086b6",
      "metadata": {
        "id": "02c6edb2-f020-413c-8bf5-6b149e0086b6"
      },
      "source": [
        "Не забываем инициализировать наш датасет"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "e4a10a88-6df0-4d1a-8640-eaca2f12e511",
      "metadata": {
        "id": "e4a10a88-6df0-4d1a-8640-eaca2f12e511"
      },
      "outputs": [],
      "source": [
        "train_dataset = HaloDataset(df_train, transform=train_transform)\n",
        "test_dataset = HaloDataset(df_test, transform=test_transform)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "adb463da-ab51-460c-bfb9-e95dddfff90f",
      "metadata": {
        "id": "adb463da-ab51-460c-bfb9-e95dddfff90f"
      },
      "source": [
        "## Переделываем модель [4 балла]\n",
        "\n",
        "В семинаре мы реализовали самый базовый детектор, а сейчас настало время его улучшать.\n",
        "\n",
        "### Backbone [1 балл]\n",
        "\n",
        "Хорошей практикой считается размораживать несколько последних слоев в backbone, это позволяет немного улучить качество модели. Давайте улушчим класс Backbone из лекции, добавив ему возможность разморозки __k__ последних слоев или блоков (на ваш выбор)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "d92b9c64-73f8-4195-aa56-b9108589a312",
      "metadata": {
        "id": "d92b9c64-73f8-4195-aa56-b9108589a312"
      },
      "outputs": [],
      "source": [
        "class Backbone(nn.Module):\n",
        "    def __init__(self, model_name=\"efficientnet_b0\", out_indices=(-3, -2, -1), unfreeze_last=3):\n",
        "        super().__init__()\n",
        "        self.backbone = timm.create_model(model_name, pretrained=True, features_only=True, out_indices=out_indices)\n",
        "        for param in self.backbone.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        params_to_unfreeze = list(self.backbone.parameters())[-unfreeze_last:]\n",
        "        for param in params_to_unfreeze:\n",
        "            param.requires_grad = True\n",
        "\n",
        "        self.feature_channels = {\n",
        "            'efficientnet_b0': [48, 120, 320, 1280],\n",
        "            'efficientnet_b1': [48, 152, 336, 1280],\n",
        "            'efficientnet_b2': [56, 184, 352, 1408],\n",
        "        }\n",
        "\n",
        "        self.model_channels = self.feature_channels.get(model_name, [64, 128, 256, 512])\n",
        "        self.out_features = ['p3', 'p4', 'p5']\n",
        "\n",
        "    def forward(self, x):\n",
        "        features_list = self.backbone(x)\n",
        "        outputs = {\n",
        "            self.out_features[i]: features_list[i]\n",
        "            for i in range(len(features_list))\n",
        "        }\n",
        "\n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b3a3748-aa11-4bf8-9b38-59ff08366573",
      "metadata": {
        "id": "3b3a3748-aa11-4bf8-9b38-59ff08366573"
      },
      "source": [
        "### NECK [2 балла]\n",
        "\n",
        "Следующее улучшение коснется шеи. Предлагаем реализовать знакомую из лекции архитектуру FPN.\n",
        "\n",
        "#### Feature Pyramid Network\n",
        "\n",
        "<center><img src=\"https://user-images.githubusercontent.com/57972646/69858594-b14a6c00-12d5-11ea-8c3e-3c17063110d3.png\"/></center>\n",
        "\n",
        "\n",
        "* [Feature Pyramid Networks for Object Detection](https://arxiv.org/abs/1612.03144)\n",
        "\n",
        "Она состоит из top-down пути, в котором происходит 2 вещи:\n",
        "1. Увеличивается пространственная размерность фичей,\n",
        "2. С помощью скипконнекшеннов, добавляются фичи из backbone модели.\n",
        "\n",
        "Для увеличения пространственной размерности используется __nearest neighbor upsampling__, а фичи из шеи и бекбоуна суммируются.\n",
        "\n",
        "__TIPS__:\n",
        "* Можете использовать базовые классы из лекции,\n",
        "* Воспользуйтесь AnchorGenerator-ом, чтобы создавать якоря сразу для нескольких выходов,\n",
        "* Не забудьте использовать nn.ModuleList, если захотите сделать динамическое количество голов у модели,\n",
        "* Также, можно добавить доп конволюцию (3х3 с паддингом) у каждого выхода шеи."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "2119ccb8-7d21-4805-af5c-5a1d8d3a0e06",
      "metadata": {
        "id": "2119ccb8-7d21-4805-af5c-5a1d8d3a0e06"
      },
      "outputs": [],
      "source": [
        "class FPNNeck(nn.Module):\n",
        "    def __init__(self, backbone_channels, out_channels=256):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        in_channels_list = backbone_channels\n",
        "\n",
        "        self.fpn = FeaturePyramidNetwork(\n",
        "            in_channels_list=in_channels_list,\n",
        "            out_channels=out_channels\n",
        "        )\n",
        "\n",
        "        self.upsample = nn.Upsample(scale_factor=2, mode='nearest')\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.pan_p4 = nn.Conv2d(out_channels, out_channels, 3, 1, 1)\n",
        "\n",
        "        self.pan_p5 = nn.Conv2d(out_channels, out_channels, 3, 1, 1)\n",
        "        self.pan_lateral_p3 = nn.Conv2d(out_channels, out_channels, 1, 1, 0)\n",
        "        self.pan_lateral_p4 = nn.Conv2d(out_channels, out_channels, 1, 1, 0)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        fpn_features = self.fpn(inputs)\n",
        "\n",
        "        c3_out = fpn_features['p3']\n",
        "        c4_out = fpn_features['p4']\n",
        "        c5_out = fpn_features['p5']\n",
        "\n",
        "        p4_pan = self.relu(self.pan_p4(c4_out))\n",
        "        p5_pan = self.relu(self.pan_p5(c5_out))\n",
        "\n",
        "        p4_up = self.upsample(p5_pan)\n",
        "        p4_pan = p4_pan + self.pan_lateral_p4(c4_out)\n",
        "        p4_pan = self.relu(p4_pan)\n",
        "\n",
        "        p3_up = self.upsample(p4_pan)\n",
        "        p3_pan = p3_up + self.pan_lateral_p3(c3_out)\n",
        "        p3_pan = self.relu(p3_pan)\n",
        "\n",
        "        return [p3_pan, p4_pan, p5_pan]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "531f59fe-8248-4b19-aa31-8acecccb0828",
      "metadata": {
        "id": "531f59fe-8248-4b19-aa31-8acecccb0828"
      },
      "source": [
        "### Head [1 балл]\n",
        "\n",
        "В качестве шеи можно выбрать __один из двух__ вариантов:\n",
        "\n",
        "#### 1. Decoupled Head\n",
        "\n",
        "Реализовать Decoupled Head из [YOLOX](https://arxiv.org/abs/2107.08430).\n",
        "<center><img src=\"https://i.ibb.co/BVtBR2R3/Decoupled-head.jpg\"/></center>\n",
        "\n",
        "**TIP**: Возьмите за основу голову из семинара, тк она сильно похожа на Decoupled Head.\n",
        "\n",
        "Изменять количество параметров у шей на разных уровнях не обязательно.\n",
        "\n",
        "#### 2. Confidence score free head\n",
        "\n",
        "Нужно взять за основу голову из семинара и полностью убрать предсказание confidence score. Чтобы модель предсказывала только 2 группы: ббоксы и классы.\n",
        "\n",
        "Есть следующие способы удаления confidence score:\n",
        "* Добавление нового класса ФОН. Обычно его обозначают нулевым классом.\n",
        "* Присваивание ббоксам БЕЗ объекта вектор из нулей в качестве таргета.\n",
        "\n",
        "Выберете тот, который вам больше нравится и будте внимательны при расчете лосса!\n",
        "\n",
        "**Важно!** Удаление confidence score повлияет на следующие методы из семинара:\n",
        "* target_assign\n",
        "* ComputeLoss\n",
        "* _filter_predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "f223b6ca-9498-4e75-9b97-6199c82977e1",
      "metadata": {
        "id": "f223b6ca-9498-4e75-9b97-6199c82977e1"
      },
      "outputs": [],
      "source": [
        "class DecoupledHead(nn.Module):\n",
        "    def __init__(self, in_channels, num_classes, num_anchors=1):\n",
        "        super().__init__()\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        self.cls_conv1 = nn.Conv2d(in_channels, in_channels, kernel_size=1, stride=1, padding=0)\n",
        "        self.cls_conv2 = nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=1, padding=1)\n",
        "        self.cls_conv3 = nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=1, padding=1)\n",
        "        self.cls_pred = nn.Conv2d(in_channels, num_anchors * num_classes, kernel_size=1, stride=1, padding=0)\n",
        "\n",
        "        self.reg_conv1 = nn.Conv2d(in_channels, in_channels, kernel_size=1, stride=1, padding=0)\n",
        "        self.reg_conv2 = nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=1, padding=1)\n",
        "        self.reg_conv3 = nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=1, padding=1)\n",
        "        self.reg_pred = nn.Conv2d(in_channels, num_anchors * 4, kernel_size=1, stride=1, padding=0)\n",
        "\n",
        "        self.obj_pred = nn.Conv2d(in_channels, num_anchors * 1, kernel_size=1, stride=1, padding=0)\n",
        "\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        cls_feat = self.relu(self.cls_conv1(x))\n",
        "        cls_feat = self.relu(self.cls_conv2(cls_feat))\n",
        "        cls_feat = self.relu(self.cls_conv3(cls_feat))\n",
        "        cls_output = self.cls_pred(cls_feat)\n",
        "\n",
        "        reg_feat = self.relu(self.reg_conv1(x))\n",
        "        reg_feat = self.relu(self.reg_conv2(reg_feat))\n",
        "        reg_feat = self.relu(self.reg_conv3(reg_feat))\n",
        "        reg_output = self.reg_pred(reg_feat)\n",
        "\n",
        "        obj_output = self.obj_pred(reg_feat)\n",
        "\n",
        "        return cls_output, reg_output, obj_output\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "88025100-78a7-4aba-b4f5-e1547b170f24",
      "metadata": {
        "id": "88025100-78a7-4aba-b4f5-e1547b170f24"
      },
      "source": [
        "Теперь можно снова реализовать класс детектора с учетом всех частей выше!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "4dd8bcb8-149f-4916-b37f-8ba0be54a4c9",
      "metadata": {
        "id": "4dd8bcb8-149f-4916-b37f-8ba0be54a4c9"
      },
      "outputs": [],
      "source": [
        "class Detector(nn.Module):\n",
        "    def __init__(self,\n",
        "                 backbone_model_name=\"efficientnet_b0\",\n",
        "                 neck_n_channels=256,\n",
        "                 num_classes=4,\n",
        "                 anchor_sizes=(32, 64, 128),\n",
        "                 anchor_ratios=(0.5, 1.0, 2.0),\n",
        "                 input_size=(640, 640),\n",
        "        ):\n",
        "        super().__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.backbone = Backbone(backbone_model_name, out_indices=(-3, -2, -1))\n",
        "        in_channels = [40, 112, 320]\n",
        "        self.neck = FPNNeck(in_channels, out_channels=neck_n_channels)\n",
        "\n",
        "        num_anchors = len(anchor_sizes) * len(anchor_ratios)\n",
        "        self.head = DecoupledHead(in_channels=neck_n_channels*3, num_anchors=num_anchors, num_classes=num_classes)\n",
        "\n",
        "        anchor_generator = AnchorGenerator(sizes=(anchor_sizes, ), aspect_ratios=(anchor_ratios, ))\n",
        "\n",
        "        reduction = self.backbone.backbone.feature_info.reduction()[0]\n",
        "        grid_sizes = [[input_size[0] // reduction, input_size[1] // reduction]]\n",
        "\n",
        "        anchors = anchor_generator.grid_anchors(grid_sizes, strides=[[reduction, reduction]])\n",
        "        anchors = torch.stack(anchors, dim=0)\n",
        "\n",
        "        anchor_centers = (anchors[:, :, :2] + anchors[:, :, 2:]) / 2\n",
        "        anchor_sizes = (anchors[:, :, 2:] - anchors[:, :, :2])\n",
        "\n",
        "        self.register_buffer(\"anchors\", anchors)\n",
        "        self.register_buffer(\"anchor_centers\", anchor_centers)\n",
        "        self.register_buffer(\"anchor_sizes\", anchor_sizes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.backbone(x)\n",
        "\n",
        "        neck_features = self.neck(features)\n",
        "\n",
        "        p3, p4, p5 = neck_features\n",
        "        p4_up = F.interpolate(p4, size=p3.shape[-2:], mode=\"nearest\")\n",
        "        p5_up = F.interpolate(p5, size=p3.shape[-2:], mode=\"nearest\")\n",
        "\n",
        "        fused = torch.cat([p3, p4_up, p5_up], dim=1)\n",
        "\n",
        "        cls_logits, bbox_preds, conf_logits = self.head(fused)\n",
        "\n",
        "        N = x.shape[0]\n",
        "        cls_logits = cls_logits.permute(0, 2, 3, 1).contiguous()\n",
        "        cls_logits = cls_logits.view(N, -1, self.head.num_classes)\n",
        "\n",
        "        bbox_preds = bbox_preds.permute(0, 2, 3, 1).contiguous()\n",
        "        bbox_preds = bbox_preds.view(N, -1, 4)\n",
        "        bbox_offsets = bbox_preds[:, :, :4]\n",
        "\n",
        "        conf_logits = conf_logits.permute(0, 2, 3, 1).contiguous()\n",
        "        conf_logits = conf_logits.view(N, -1, 1)\n",
        "        confidence_logits = conf_logits[:, :, 0]\n",
        "\n",
        "\n",
        "        if self.training:\n",
        "            return bbox_offsets, confidence_logits, cls_logits\n",
        "\n",
        "\n",
        "        bboxes = self.decode_bboxes(bbox_offsets)\n",
        "        confidence = torch.sigmoid(confidence_logits)\n",
        "        cls_probs = torch.softmax(cls_logits, dim=-1)\n",
        "        return bboxes, confidence, cls_probs\n",
        "\n",
        "\n",
        "    def decode_bboxes(self, bbox_offsets):\n",
        "        tx = bbox_offsets[:, :, 0]\n",
        "        ty = bbox_offsets[:, :, 1]\n",
        "        tw = bbox_offsets[:, :, 2]\n",
        "        th = bbox_offsets[:, :, 3]\n",
        "\n",
        "        center_x = self.anchor_centers[:, :, 0] + torch.sigmoid(tx) * self.anchor_sizes[:, :, 0]\n",
        "        center_y = self.anchor_centers[:, :, 1] + torch.sigmoid(ty) * self.anchor_sizes[:, :, 1]\n",
        "\n",
        "        w = torch.exp(tw) * self.anchor_sizes[:, :, 0]\n",
        "        h = torch.exp(th) * self.anchor_sizes[:, :, 1]\n",
        "\n",
        "        x_min = center_x - w / 2\n",
        "        y_min = center_y - h / 2\n",
        "        return torch.stack([x_min, y_min, w, h], dim=-1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def safe_logit(x):\n",
        "    \"\"\" Безопасный расчет logit'ов. \"\"\"\n",
        "    eps = 1e-6\n",
        "    x = torch.clamp(x, eps, 1 - eps)\n",
        "    return torch.log(x / (1 - x))\n",
        "\n",
        "def get_target_offset(anchor_box, gt_box):\n",
        "    \"\"\" Расчитываем таргет как желаемые смещения от якорей до GT.\n",
        "\n",
        "    anchor_box: torch.Tensor в формате (x_min, y_min, x_max, y_max),\n",
        "    gt_box: torch.Tensor в формате (x_min, y_min, x_max, y_max).\n",
        "    \"\"\"\n",
        "\n",
        "    gt_center = (gt_box[:2] + gt_box[2:]) / 2\n",
        "    gt_size = gt_box[2:] - gt_box[:2]\n",
        "\n",
        "\n",
        "    anchor_center = (anchor_box[:2] + anchor_box[2:]) / 2\n",
        "    anchor_size = anchor_box[2:] - anchor_box[:2]\n",
        "\n",
        "    tx = (gt_center[0] - anchor_center[0]) / anchor_size[0]\n",
        "    ty = (gt_center[1] - anchor_center[1]) / anchor_size[1]\n",
        "    target_tx = safe_logit(tx)\n",
        "    target_ty = safe_logit(ty)\n",
        "\n",
        "    target_tw = torch.log(gt_size[0] / anchor_size[0])\n",
        "    target_th = torch.log(gt_size[1] / anchor_size[1])\n",
        "    return torch.tensor([target_tx, target_ty, target_tw, target_th]).to(anchor_box.device)"
      ],
      "metadata": {
        "id": "4CKDx_3OrZdW"
      },
      "id": "4CKDx_3OrZdW",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def assign_target(anchors, gt_boxes, gt_labels, num_classes, pos_th=0.6, neg_th=0.3):\n",
        "\n",
        "    num_anchors = anchors.shape[0]\n",
        "    target_objectness = torch.zeros(num_anchors, device=anchors.device)\n",
        "    target_offsets = torch.zeros((num_anchors, 4), device=anchors.device)\n",
        "    target_cls = torch.zeros((num_anchors, num_classes), device=anchors.device)\n",
        "\n",
        "    if gt_boxes.numel() == 0:\n",
        "        return target_offsets, target_objectness, target_cls\n",
        "\n",
        "    gt_xyxy = gt_boxes.clone()\n",
        "    gt_xyxy[:, 2:] = gt_xyxy[:, :2] + gt_xyxy[:, 2:]\n",
        "\n",
        "    ious = box_iou(anchors, gt_xyxy)\n",
        "\n",
        "    best_iou, best_gt_idx = ious.max(dim=1)\n",
        "\n",
        "    ignore_mask = (best_iou >= neg_th) & (best_iou < pos_th)\n",
        "    target_objectness[ignore_mask] = -1\n",
        "\n",
        "    pos_mask = best_iou >= pos_th\n",
        "    pos_indices = pos_mask.nonzero(as_tuple=True)[0]\n",
        "    for pos in pos_indices:\n",
        "        gt_idx = best_gt_idx[pos]\n",
        "        gt_box = gt_xyxy[gt_idx]\n",
        "        anchor_box = anchors[pos]\n",
        "\n",
        "        target_offsets[pos] = get_target_offset(anchor_box, gt_box)\n",
        "        target_objectness[pos] = 1\n",
        "        target_cls[pos, gt_labels[gt_idx]] = 1\n",
        "\n",
        "    for gt_idx in range(gt_xyxy.shape[0]):\n",
        "        if not((target_objectness == 1) & (best_gt_idx == gt_idx)).any():\n",
        "            best_anchor_idx = torch.argmax(ious[:, gt_idx])\n",
        "            target_offsets[best_anchor_idx] = get_target_offset(anchors[best_anchor_idx], gt_xyxy[gt_idx])\n",
        "            target_objectness[best_anchor_idx] = 1\n",
        "            target_cls[best_anchor_idx, gt_labels[gt_idx]] = 1\n",
        "    return target_offsets, target_objectness, target_cls"
      ],
      "metadata": {
        "id": "ATyJ9zoVrdkw"
      },
      "id": "ATyJ9zoVrdkw",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ComputeLoss:\n",
        "    \"\"\" Базовый расчет лосса.\n",
        "\n",
        "    Параметры\n",
        "    ---------\n",
        "    bbox_loss : Локализационная часть лосса\n",
        "    obj_loss : Лосс для Confidence score\n",
        "    cls_loss : Классификационная часть лосса\n",
        "    weight_bbox, weight_obj, weight_cls : Константы для баллансировки частей лосса\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "            bbox_loss=None, obj_loss=None, cls_loss=None,\n",
        "            weight_bbox=5, weight_obj=1, weight_cls=1\n",
        "        ):\n",
        "        self.bbox_loss = nn.SmoothL1Loss() if bbox_loss is None else bbox_loss\n",
        "        self.obj_loss = nn.BCEWithLogitsLoss() if obj_loss is None else obj_loss\n",
        "        self.cls_loss = nn.BCEWithLogitsLoss() if cls_loss is None else cls_loss\n",
        "        self.weight_bbox = weight_bbox\n",
        "        self.weight_obj = weight_obj\n",
        "        self.weight_cls = weight_cls\n",
        "\n",
        "    def __call__(self, predicts, targets):\n",
        "        \"\"\" Расчет лосса для пары (предсказание, таргет)\n",
        "\n",
        "        Параметры\n",
        "        ---------\n",
        "        predicts : Предсказания модели для одной картинки: Смещения, objectness score и логиты для классов\n",
        "        targets : Gt значения для расчета лосса, а именно: GT смещения, GT objectness score и GT ohe классы\n",
        "        \"\"\"\n",
        "        pred_offsets, pred_obj_logits, pred_cls_logits = predicts\n",
        "        target_boxes, target_obj, target_cls = targets\n",
        "        # Confidence score считается только для предсказаний соотв отрицательным и положительным якорям\n",
        "        valid_mask = target_obj != -1\n",
        "        loss_obj = self.obj_loss(pred_obj_logits[valid_mask], target_obj[valid_mask])\n",
        "\n",
        "        # Локализационная и классификационные части считаются для предсказаинй соотв положительным якорям\n",
        "        pos_mask = target_obj == 1\n",
        "        if pos_mask.sum() > 0:\n",
        "            loss_cls = self.cls_loss(pred_cls_logits[pos_mask], target_cls[pos_mask])\n",
        "            loss_bbox = self.bbox_loss(pred_offsets[pos_mask], target_boxes[pos_mask])\n",
        "        else:\n",
        "            loss_cls = torch.tensor(0.0, device=pred_offsets.device)\n",
        "            loss_bbox = torch.tensor(0.0, device=pred_offsets.device)\n",
        "        return self.weight_bbox * loss_bbox + self.weight_obj * loss_obj + self.weight_cls * loss_cls"
      ],
      "metadata": {
        "id": "ne5OesIrrk4j"
      },
      "id": "ne5OesIrrk4j",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Runner:\n",
        "    \"\"\" Базовый класс для обучения и валидации модели.\n",
        "\n",
        "    Параметры\n",
        "    ---------\n",
        "    model : torch модель, которая будет обучаться.\n",
        "    compute_loss : экземпляр класса ComputeLoss (или другого с реализованным методом __call__).\n",
        "    optimizer : torch optimizer\n",
        "    train_dataloader : torch dataloader семплирующий данные для обучения модели.\n",
        "    assign_target_method : callable, который решает задачу сопоставления якорей и таргета (например, assign_target)\n",
        "    deivce : девайс на котором будет происходить обучения, по дефолту \"cpu\"\n",
        "    scheduler : torch scheduler\n",
        "    assign_target_kwargs : доп параметры для функции в `assign_target_method`,\n",
        "    val_dataloader : torch dataloader загружающий валидационные данные.\n",
        "    score_threshold : При расчете метрики на валидации, все предсказания,\n",
        "        с (confidence score * cls_probs) < score_threshold будут проигнорированны.\n",
        "    nms_threshold : Предсказания, имеющие пересечение по IoU >= nms_threshold будут считаться одним предсказанием.\n",
        "    max_boxes_per_cls : Максимальное количество ббоксов на изображение для одного класса после фильтрации по `score_threshold`.\n",
        "    \"\"\"\n",
        "    def __init__(self, model, compute_loss, optimizer, train_dataloader, assign_target_method, device=None,\n",
        "                 scheduler=None, assign_target_kwargs=None,\n",
        "                 val_dataloader=None, val_every=1, score_threshold=0.1, nms_threshold=0.5, max_boxes_per_cls=8):\n",
        "        self.model = model\n",
        "        self.compute_loss = compute_loss\n",
        "        self.optimizer = optimizer\n",
        "        self.train_dataloader = train_dataloader\n",
        "        assign_target_kwargs = {} if assign_target_kwargs is None else assign_target_kwargs\n",
        "        self.assign_target_method = partial(assign_target_method, **assign_target_kwargs)\n",
        "        self.device = \"cpu\" if device is None else device\n",
        "        self.scheduler = scheduler\n",
        "\n",
        "        # Валидационные параметры\n",
        "        self.val_dataloader = val_dataloader\n",
        "        self.val_every = val_every\n",
        "        self.score_threshold = score_threshold\n",
        "        self.nms_threshold = nms_threshold\n",
        "        self.max_boxes_per_cls = max_boxes_per_cls\n",
        "\n",
        "        # Вспомогательные массивы\n",
        "        self.batch_loss = []\n",
        "        self.epoch_loss = []\n",
        "        self.val_metric = []\n",
        "\n",
        "    def _run_train_epoch(self, dataloader, verbose=True):\n",
        "        \"\"\" Обучить модель одну эпоху на данных из `dataloader` \"\"\"\n",
        "        self.model.train()\n",
        "        batch_loss = []\n",
        "        scaler = torch.amp.GradScaler('cuda')\n",
        "        for images, targets in (pbar := tqdm(dataloader, desc=f\"Process train epoch\", leave=False)):\n",
        "            images = images.to(self.device)\n",
        "            with torch.amp.autocast('cuda', dtype=torch.float16):\n",
        "                outputs = self.model(images)\n",
        "\n",
        "                anchors = self.model.anchors.view(-1, 4)\n",
        "                accum_loss = 0.0\n",
        "                for ix in range(images.shape[0]):\n",
        "                    gt_boxes = targets[ix]['boxes'].to(self.device)\n",
        "                    gt_labels = targets[ix]['labels'].to(self.device)\n",
        "                    # выбираем какие якоря будут использоваться при расчете лосса.\n",
        "                    assigned_targets = self.assign_target_method(anchors, gt_boxes, gt_labels,\n",
        "                                                                 num_classes=model.num_classes)\n",
        "                    # Считаем лосс на основании предсказаний модели и таргетов.\n",
        "                    outputs_ixs = [out[ix] for out in outputs]\n",
        "                    loss = self.compute_loss(outputs_ixs, assigned_targets)\n",
        "                    accum_loss += loss\n",
        "            accum_loss = accum_loss / images.shape[0]\n",
        "            batch_loss.append(accum_loss.cpu().detach().item())\n",
        "\n",
        "            # Делаем шаг оптимизатора после расчета лосса для всех элементов батча\n",
        "            self.optimizer.zero_grad()\n",
        "            #accum_loss.backward()\n",
        "            scaler.scale(accum_loss).backward()\n",
        "            #self.optimizer.step()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "        # Обновляем описание tqdm бара усредненным значением лосса за предыдущй батч\n",
        "            if verbose:\n",
        "                pbar.set_description(f\"Current batch loss: {batch_loss[-1]:.4}\")\n",
        "        return batch_loss\n",
        "\n",
        "    def train(self, num_epochs=10, verbose=True):\n",
        "        \"\"\" Обучаем модель заданное количество эпох. \"\"\"\n",
        "        val_desc = \"\"\n",
        "        for epoch in (epoch_pbar := tqdm(range(1, num_epochs+1), desc=\"Train epoch\", total=num_epochs)):\n",
        "            # Обучаем модель одну эпоху\n",
        "            loss = self._run_train_epoch(self.train_dataloader, verbose=verbose)\n",
        "            self.batch_loss.extend(loss)\n",
        "            self.epoch_loss.append(np.mean(self.batch_loss[-len(self.train_dataloader):]))\n",
        "\n",
        "            # Делаем валидацию, если был передан валидационный датасет\n",
        "            if self.val_dataloader is not None and epoch % self.val_every == 0:\n",
        "                val_metric = self.validate()\n",
        "                self.val_metric.append(val_metric)\n",
        "                val_desc = f\" Val {val_metric:.4}\"\n",
        "\n",
        "            # Обновляем описание tqdm бара усредненным значением лосса за предыдую эпоху\n",
        "            if verbose:\n",
        "                epoch_pbar.set_description(f\"Last epoch loss: Train {self.epoch_loss[-1]:.4}\" + val_desc)\n",
        "            # Делаем шаг scheduler'a если он был передан\n",
        "            if self.scheduler is not None:\n",
        "                self.scheduler.step()\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def validate(self, dataloader=None):\n",
        "        \"\"\" Метод для валидации модели. Если dataloader не передан, будет использоваться self.val_dataloder.\n",
        "        Возвращает mAP (0.5 ... 0.95).\n",
        "        \"\"\"\n",
        "        self.model.eval()\n",
        "        dataloader = self.val_dataloader if dataloader is None else dataloader\n",
        "        # Считаем метрику mAP с помощью функции из torchmetrics\n",
        "        metric = MeanAveragePrecision(box_format=\"xywh\", iou_type=\"bbox\")\n",
        "        for images, targets in tqdm(dataloader, desc=\"Running validation\", leave=False):\n",
        "            images = images.to(self.device)\n",
        "            outputs = self.model(images)\n",
        "            predicts = _filter_predictions(outputs, self.score_threshold, self.nms_threshold,\n",
        "                                           max_boxes_per_cls=self.max_boxes_per_cls, return_type=\"torch\")\n",
        "            metric.update(predicts, targets)\n",
        "        return metric.compute()[\"map\"].item()\n",
        "\n",
        "    def plot_loss(self, row_figsize=3):\n",
        "        nrows = 2 if self.val_metric else 1\n",
        "        _, ax = plt.subplots(nrows, 1, figsize=(12, row_figsize*nrows), tight_layout=True)\n",
        "        ax = np.array([ax]) if not isinstance(ax, np.ndarray) else ax\n",
        "        ax[0].plot(self.batch_loss, label=\"Train batch Loss\", color=\"tab:blue\")\n",
        "        ax[0].plot(np.arange(1, len(self.batch_loss)+1, len(self.train_dataloader)), self.epoch_loss,\n",
        "                   color=\"tab:orange\", label=\"Train epoch Loss\")\n",
        "        ax[0].grid()\n",
        "        ax[0].set_title(\"Train Loss\")\n",
        "        ax[0].set_xlabel(\"Number of Iterations\")\n",
        "        ax[0].set_ylabel(\"Loss\")\n",
        "        #print(self.val_every, len(self.batch_loss)+1, len(self.val_dataloader) * self.val_every)\n",
        "        #print(np.array(self.val_metric) * 100)\n",
        "        if self.val_metric:\n",
        "            ax[1].plot(np.arange(self.val_every, num_epochs+1, self.val_every),\n",
        "                       np.array(self.val_metric) * 100, color=\"tab:green\", label=\"Validation mAP\")\n",
        "            ax[1].grid()\n",
        "            ax[1].set_title(\"Validation mAP\")\n",
        "            ax[1].set_xlabel(\"Number of Iterations\")\n",
        "            ax[1].set_ylabel(\"mAP (%)\")\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "\n",
        "def _filter_predictions(predictions, score_threshold=0.1, nms_threshold=0.5, max_boxes_per_cls=8, return_type=\"list\"):\n",
        "    \"\"\" Ббоксы в `predictions` должны быть в формате (x_min, y_min, w, h). \"\"\"\n",
        "    # Итоговый скор считается как произведение уверенности модели в том что в данном якоре\n",
        "    # и вероятность каждого класса в данном якоре.\n",
        "    bboxes, confidences, cls_probs = predictions\n",
        "    all_final_scores = confidences[:, :, None] * cls_probs\n",
        "\n",
        "    num_classes = cls_probs.shape[-1]\n",
        "    final_predictions = []\n",
        "    # Для каждого элемента в `predictions` независимо выбираем ббоксы и скоры\n",
        "    for boxes, final_scores in zip(bboxes, all_final_scores):\n",
        "        preds = {\"boxes\": [], \"labels\": [], \"scores\": []}\n",
        "\n",
        "        # Для каждого класса отдельно фильтруем ббоксы с помощью NMS\n",
        "        for cls in range(num_classes):\n",
        "            cls_scores = final_scores[:, cls]\n",
        "            # Фильтруем ббоксы, score которых меньше порога\n",
        "            keep_ixs = cls_scores > score_threshold\n",
        "            if keep_ixs.sum() == 0:\n",
        "                continue\n",
        "            cls_boxes = boxes[keep_ixs]\n",
        "            cls_scores = cls_scores[keep_ixs]\n",
        "\n",
        "            # Если предсказаний слишком много, выбираем только самые уверенные\n",
        "            if len(cls_boxes) > max_boxes_per_cls:\n",
        "                pos = torch.argsort(cls_scores, descending=True)\n",
        "                cls_boxes = cls_boxes[pos[:max_boxes_per_cls]]\n",
        "                cls_scores = cls_scores[pos[:max_boxes_per_cls]]\n",
        "\n",
        "            # Конвертируем ббоксы в формат x_min, y_min, x_max, y_max\n",
        "            boxes_xyxy = cls_boxes.clone()\n",
        "            boxes_xyxy[:, 2:] = boxes_xyxy[:, :2] + boxes_xyxy[:, 2:]\n",
        "            # Запускаем NMS по всем оставшимся ббоксам класса cls\n",
        "            pred_ixs = nms(boxes_xyxy, cls_scores, nms_threshold)\n",
        "            # Сохраняем все предсказания для класса cls\n",
        "            for ix in pred_ixs:\n",
        "                preds[\"boxes\"].append(cls_boxes[ix].cpu().tolist())\n",
        "                preds[\"labels\"].append(cls)\n",
        "                preds[\"scores\"].append(cls_scores[ix].item())\n",
        "        if return_type == \"torch\":\n",
        "            for key, item in preds.items():\n",
        "                preds[key] = torch.tensor(item)\n",
        "        elif return_type != \"list\":\n",
        "            raise ValueError(f\"Received unexpected `return_type`. Could be either `torch` or `list`, not {return_type}\")\n",
        "        final_predictions.append(preds)\n",
        "    return final_predictions"
      ],
      "metadata": {
        "id": "PIUXTq_frmQU"
      },
      "id": "PIUXTq_frmQU",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True, collate_fn=collate_fn)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=8, shuffle=False, collate_fn=collate_fn)"
      ],
      "metadata": {
        "id": "vMR17cuYrnkD"
      },
      "id": "vMR17cuYrnkD",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "lr = 1e-3\n",
        "\n",
        "model = Detector(\"efficientnet_b0\", num_classes=4, anchor_sizes=(30, 50, 140, 300), anchor_ratios=(0.5, 1, 1.6, 2)).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10, eta_min=5e-5)\n",
        "\n",
        "smooth_l1_loss = nn.SmoothL1Loss()\n",
        "bce_loss = nn.BCEWithLogitsLoss()\n",
        "compute_loss = ComputeLoss(smooth_l1_loss, bce_loss, bce_loss, weight_bbox=10)\n",
        "\n",
        "runner = Runner(model, compute_loss, optimizer, train_dataloader, assign_target, device=device,\n",
        "                 scheduler=scheduler, assign_target_kwargs={\"neg_th\":0.4, \"pos_th\":0.6},\n",
        "                 val_dataloader=test_dataloader)\n",
        "\n"
      ],
      "metadata": {
        "id": "zDWPIRiuro2c",
        "outputId": "52016cdd-0186-4552-e19c-ec696165e938",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190,
          "referenced_widgets": [
            "158f2ea4fe3340e49afdd9d3475c43bf",
            "95868ce8350d48aba18da8d072d291d0",
            "0c8cf014acda4e169082c520aec0ce97",
            "f02110be3bcb4a579062a623c0affe56",
            "2c2b771d29ee423288a88d5cbf2be9e9",
            "1c8a8094b6a74122a2d50b3eefa60b93",
            "d356825c65e04a4493fb63d9a7950924",
            "edf9cb94ae2c4f3697019da5b690dd99",
            "b96949a7fff64050a5c1ec5cc008424e",
            "c1207abc62fd473eb3cbb68a7767d85e",
            "05e418e1d7fa45fc82c28bc2a6ee4af0"
          ]
        }
      },
      "id": "zDWPIRiuro2c",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/21.4M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "158f2ea4fe3340e49afdd9d3475c43bf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:timm.models._builder:Unexpected keys (bn2.num_batches_tracked, bn2.bias, bn2.running_mean, bn2.running_var, bn2.weight, classifier.bias, classifier.weight, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()\n",
        "import gc\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "JoyK5nbDrqeZ",
        "outputId": "82715489-f159-454d-83dc-484aa3d20bec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "JoyK5nbDrqeZ",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "337"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 100"
      ],
      "metadata": {
        "id": "_ZH3SOa7rsI4"
      },
      "id": "_ZH3SOa7rsI4",
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "runner.train(num_epochs=num_epochs, verbose=True)"
      ],
      "metadata": {
        "id": "qdauIyG3ruSK",
        "outputId": "8909782b-2a08-48c3-f841-72e98a49977d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351,
          "referenced_widgets": [
            "e2d2c43a7ac24dedbcb6d8329e54eb6f",
            "59cdb944fdb94ec78444b90cb45f7aed",
            "3b45fa25c265461c86eaf94dd3357f0c",
            "6e6a81e5d7704cfba413c13df93f4ce1",
            "1168d40bb8d84aab9f9d59c6bc806b21",
            "a17025e0c8e445d48e38e4ef7d49f6ef",
            "3f2f9afdc19c4f58b9f3b871e05abc14",
            "39686ce4303944a89bc8e6b7006cb17f",
            "896a152fc4af461d9458cf2ca4b504d2",
            "40954eebb58e4e08ae6a51246fda6e2f",
            "1921992aca6d4abaa2d8c8983de12f76",
            "02bc60d1620147f69614ee4f10cdc565",
            "452bda554ebf46ea81e0e09bda05adac",
            "45c34ab41be9440fa539026f0b284774",
            "0f52dc57dd4d44c19f3d652c4da8654f",
            "71962b499338466daace74a202affe35",
            "6953d937b05944419ab7d79422d56172",
            "eb833deab8e146959c23046d2df221a6",
            "82b019971a5e4aebb60818aaac66e9ae",
            "bec7f6596ab644eaac68a17304112ef8",
            "e1740bc35ff2404d89dc8b2ac0fdb54a",
            "5b1d7bbdc3c24a97ad716338d9842ac0"
          ]
        }
      },
      "id": "qdauIyG3ruSK",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Train epoch:   0%|          | 0/100 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e2d2c43a7ac24dedbcb6d8329e54eb6f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Process train epoch:   0%|          | 0/8 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "02bc60d1620147f69614ee4f10cdc565"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "The shape of the mask [102400] at index 0 does not match the shape of the indexed tensor [1024] at index 0",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4012162470.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-2802902396.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, num_epochs, verbose)\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mepoch_pbar\u001b[0m \u001b[0;34m:=\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Train epoch\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0;31m# Обучаем модель одну эпоху\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_train_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_loss\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2802902396.py\u001b[0m in \u001b[0;36m_run_train_epoch\u001b[0;34m(self, dataloader, verbose)\u001b[0m\n\u001b[1;32m     62\u001b[0m                     \u001b[0;31m# Считаем лосс на основании предсказаний модели и таргетов.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m                     \u001b[0moutputs_ixs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mix\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mout\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m                     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs_ixs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0massigned_targets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m                     \u001b[0maccum_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0maccum_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccum_loss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1772857759.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, predicts, targets)\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;31m# Confidence score считается только для предсказаний соотв отрицательным и положительным якорям\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mvalid_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_obj\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mloss_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_obj_logits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalid_mask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_obj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalid_mask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;31m# Локализационная и классификационные части считаются для предсказаинй соотв положительным якорям\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: The shape of the mask [102400] at index 0 does not match the shape of the indexed tensor [1024] at index 0"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "runner.plot_loss(row_figsize=4)"
      ],
      "metadata": {
        "id": "ZYU_NO1BryDE"
      },
      "id": "ZYU_NO1BryDE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_iter = iter(test_dataloader)"
      ],
      "metadata": {
        "id": "SarUjx48r1UF"
      },
      "id": "SarUjx48r1UF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_to_color = {\n",
        "    1: (89, 161, 197),\n",
        "    2: (204, 79, 135),\n",
        "    3: (125, 216, 93),\n",
        "    4: (175, 203, 33),\n",
        "}\n",
        "\n",
        "class_to_name = {\n",
        "    1 : \"enemy\",\n",
        "    2 : \"enemy-head\",\n",
        "    3 : \"friendly\",\n",
        "    4 : \"friendly-head\"\n",
        "}"
      ],
      "metadata": {
        "id": "hH_tbjOar2RT"
      },
      "id": "hH_tbjOar2RT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "J6hn1fXyr3f8"
      },
      "id": "J6hn1fXyr3f8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zAli2KsIr5P2"
      },
      "id": "zAli2KsIr5P2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def predict(model, images, device, score_threshold=0.1, nms_threshold=0.5, max_boxes_per_cls=8, return_type='list'):\n",
        "    \"\"\" Предсказание моделью для переданного набора изображений после фильтрации по score_threshold\n",
        "    и применения NMS.\n",
        "\n",
        "    Параметры\n",
        "    --------\n",
        "    images : torch.tensor, содержащий картинки для которых нужно сделать предсказание.\n",
        "    Необходимые преобразования должны быть сделаны ДО. Внутри метода `predict` никаких преобразований\n",
        "    не происходит.\n",
        "    score_threshold : Все предсказания, с (confidence score * cls_probs) < score_threshold будут проигнорированны.\n",
        "    nms_threshold : Предсказания, имеющие пересечение по IoU >= nms_threshold будут считаться одним предсказанием.\n",
        "    max_boxes_per_cls : Максимальное количество ббоксов на изображение для одного класса после фильтрации по `score_threshold`.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    final_predictions : List[dict], где каждый словарь содержащий следующие ключи:\n",
        "        \"boxes\" : координаты ббоксов на i-ом изображении,\n",
        "        \"labels\" : классы внутри ббоксов,\n",
        "        \"scores\" : Confidence scores для ббоксов.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    images = images.to(device)\n",
        "    outputs = model(images)\n",
        "    final_predictions =  _filter_predictions(outputs, score_threshold=score_threshold, nms_threshold=nms_threshold,\n",
        "                                             max_boxes_per_cls=max_boxes_per_cls, return_type=return_type)\n",
        "    return final_predictions"
      ],
      "metadata": {
        "id": "Xugp927urz8L"
      },
      "id": "Xugp927urz8L",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Вспомогательные функции для отрисовки данных\n",
        "def add_bbox(image, box, label='', color=(128, 128, 128), txt_color=(0, 0, 0)):\n",
        "    lw = max(round(sum(image.shape) / 2 * 0.003), 2)\n",
        "    p1, p2 = (int(box[0]), int(box[1])), (int(box[0]) + int(box[2]), int(box[1]) + int(box[3]))\n",
        "    cv2.rectangle(image, p1, p2, color, thickness=lw, lineType=cv2.LINE_AA)\n",
        "    if label:\n",
        "        tf = max(lw - 1, 1)\n",
        "        w, h = cv2.getTextSize(label, 0, fontScale=lw / 3, thickness=tf)[0]\n",
        "        outside = p1[1] - h >= 3\n",
        "        p2 = p1[0] + w, p1[1] - h - 3 if outside else p1[1] + h + 3\n",
        "        cv2.rectangle(image, p1, p2, color, -1, cv2.LINE_AA)\n",
        "        cv2.putText(image,\n",
        "                    label, (p1[0], p1[1] - 2 if outside else p1[1] + h + 2),\n",
        "                    0,\n",
        "                    lw / 3,\n",
        "                    txt_color,\n",
        "                    thickness=tf,\n",
        "                    lineType=cv2.LINE_AA)\n",
        "    return image\n",
        "\n",
        "def plot_examples(df, indices=None, num_examples=6, row_figsize=(12, 3)):\n",
        "    if indices is None:\n",
        "        indices = np.random.choice(len(df), size=num_examples, replace=False)\n",
        "    else:\n",
        "        num_examples = len(indices)\n",
        "    ncols = min(num_examples, 3)\n",
        "    nrows = math.ceil(num_examples / 3)\n",
        "    _, axes = plt.subplots(nrows, ncols, figsize=(row_figsize[0], row_figsize[1] * nrows), tight_layout=True)\n",
        "    axes = axes.reshape(-1)\n",
        "    for ix, ax in zip(indices, axes):\n",
        "        row = df.iloc[ix]\n",
        "        image = Image.open(io.BytesIO(row['image']['bytes']))\n",
        "        bboxes = row[\"objects\"]['bbox']\n",
        "        classes = row[\"objects\"]['category']\n",
        "        img = np.array(image)\n",
        "        for bbox, label in zip(bboxes, classes):\n",
        "            color = class_to_color[label]\n",
        "            class_name = class_to_name[label]\n",
        "            img = add_bbox(img, bbox, label=str(class_name), color=color)\n",
        "        ax.imshow(img)\n",
        "        ax.set_title(f\"Image id: {row['image_id']}\")\n",
        "        ax.set_xticks([])\n",
        "        ax.set_yticks([])"
      ],
      "metadata": {
        "id": "AA1pMMlir8F2"
      },
      "id": "AA1pMMlir8F2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_predictions(images, predictions, figsize=(12, 3)):\n",
        "    \"\"\" Рисуем по 3 предсказания на одной строке. \"\"\"\n",
        "    ncols = min(len(images), 3)\n",
        "    for ix in range(0, len(images), ncols):\n",
        "        _, axes = plt.subplots(1, ncols, figsize=figsize, tight_layout=True)\n",
        "        for i, (ax, img) in enumerate(zip(axes, images[ix: ix+ncols])):\n",
        "            img = img.cpu().permute(1, 2, 0).numpy()\n",
        "            img = img * np.array(std).reshape(1, 1, -1) + np.array(mean).reshape(1, 1, -1)\n",
        "            img = np.ascontiguousarray((img * 255).astype(np.uint8))\n",
        "            preds = predictions[ix + i]\n",
        "            for bbox, label, score in zip(preds[\"boxes\"], preds[\"labels\"], preds[\"scores\"]):\n",
        "                color = class_to_color[label+1]\n",
        "                label = class_to_name[label+1]\n",
        "                img = add_bbox(img, bbox, label=f\"Class {label}: {score:.2f}\", color=color)\n",
        "            ax.imshow(img)\n",
        "            ax.set_xticks([])\n",
        "            ax.set_yticks([])\n",
        "        plt.show()\n",
        "    plt.close()"
      ],
      "metadata": {
        "id": "iZ12uU9Zr6_1"
      },
      "id": "iZ12uU9Zr6_1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "9bc104b6-0960-404d-805c-ec880dc9bfb0",
      "metadata": {
        "id": "9bc104b6-0960-404d-805c-ec880dc9bfb0"
      },
      "source": [
        "## Label assignment [3 балла]\n",
        "В этой секции предлагается заменить функцию `assign_target` на более современный алгоритм который называется Task alignment learning.\n",
        "\n",
        "Он описан в статье [TOOD](https://arxiv.org/abs/2108.07755) в секции 3.2. Для удобства вот его основные шаги:\n",
        "\n",
        "1. Посчитать значение метрики для каждого предсказанного ббокса:\n",
        "    \n",
        "$$t = s^\\alpha * u^\\beta$$\n",
        "    \n",
        "где,\n",
        "* $s$ — classification score, или вероятность принадлежности предсказанного ббокса к классу реального ббокса (**GT**);\n",
        "* $u$ — IoU между предсказанным и реальным ббоксами;\n",
        "* $\\alpha,\\ \\beta$ — нормализационные константы, обычно $\\alpha = 6.0, \\ \\beta = 1.0$.\n",
        "    \n",
        "2. Отфильтровать предсказания на основе **GT**.\n",
        "\n",
        "    Для якорных детекторов, обычно, выбираются только те предсказания, центры якорей которых находятся внутри GT.\n",
        "4. Для каждого **GT** выбрать несколько (обычно 5 или 13) самых подходящих предсказаний.\n",
        "5. Если предсказание рассматривается в качестве подходящего для нескольких **GT** — выбрать **GT** с наибольшим пересечением по IoU.\n",
        "\n",
        "\n",
        "**BAЖНО**: если будете использовать Runner из лекции, не забудьте поменять параметры  в `self.assign_target_method` в методе `_run_train_epoch`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a88e418e-2fd5-4f7b-9675-f91db3c52ec5",
      "metadata": {
        "id": "a88e418e-2fd5-4f7b-9675-f91db3c52ec5"
      },
      "outputs": [],
      "source": [
        "def TAL_assigner(pred_scores, pred_bboxes, anchors, gt_labels, gt_bboxes,\n",
        "                topk=5, alpha=6.0, beta=1.0, eps=1e-9):\n",
        "    \"\"\"\n",
        "    TAL assigner для batch_size=1.\n",
        "\n",
        "    Args:\n",
        "        pred_scores: (num_anchors, num_classes)\n",
        "        pred_bboxes: (num_anchors, 4) xyxy\n",
        "        anchors: (num_anchors, 2) центры якорей\n",
        "        gt_labels: (n_gt,)\n",
        "        gt_bboxes: (n_gt, 4) xyxy\n",
        "        topk: количество кандидатов на GT\n",
        "    \"\"\"\n",
        "    device = pred_scores.device\n",
        "    num_anchors, num_classes = pred_scores.shape\n",
        "    n_gt = len(gt_bboxes)\n",
        "\n",
        "    # 1. Фильтрация: якоря внутри GT\n",
        "    mask_in_gts = _select_candidates_in_gts(anchors, gt_bboxes)\n",
        "\n",
        "    # 2. Метрика t = s^α * u^β\n",
        "    align_metric, overlaps = _compute_align_metric(\n",
        "        pred_scores, pred_bboxes, gt_labels, gt_bboxes, mask_in_gts\n",
        "    )\n",
        "\n",
        "    # 3. Top-k для каждого GT\n",
        "    mask_topk = _select_topk_candidates(align_metric, topk)\n",
        "\n",
        "    # 4. Финальная маска положительных\n",
        "    mask_pos = mask_topk * mask_in_gts\n",
        "\n",
        "    # 5. Разрешение конфликтов (max IoU)\n",
        "    target_gt_idx, fg_mask, mask_pos = _select_highest_overlaps(mask_pos, overlaps, n_gt)\n",
        "\n",
        "    # 6. Targets\n",
        "    target_labels, target_bboxes, target_scores = _get_targets(\n",
        "        gt_labels, gt_bboxes, target_gt_idx, fg_mask\n",
        "    )\n",
        "\n",
        "    # 7. Нормализация по TAL\n",
        "    target_scores = _normalize_target_scores(target_scores, align_metric, overlaps, mask_pos)\n",
        "\n",
        "    return target_labels, target_bboxes, target_scores, fg_mask.bool()\n",
        "\n",
        "\n",
        "def _select_candidates_in_gts(anchors, gt_bboxes):\n",
        "    \"\"\"Якоря внутри GT: центр якоря в GT bbox.\"\"\"\n",
        "    lt, rb = gt_bboxes[:, :2], gt_bboxes[:, 2:]\n",
        "    bbox_deltas = torch.cat((anchors[None] - lt[:, None], rb[:, None] - anchors[None]), dim=-1)\n",
        "    return bbox_deltas.min(-1).gt_(1e-9)  # (n_gt, num_anchors)\n",
        "\n",
        "\n",
        "def _compute_align_metric(pred_scores, pred_bboxes, gt_labels, gt_bboxes, mask_in_gts):\n",
        "    \"\"\"t = s^α * u^β.\"\"\"\n",
        "    na, nc = pred_scores.shape\n",
        "    n_gt = len(gt_bboxes)\n",
        "\n",
        "    # Classification scores для GT классов\n",
        "    bbox_scores = pred_scores[gt_labels]  # (n_gt, na)\n",
        "\n",
        "    # IoU pred vs GT\n",
        "    overlaps = bbox_iou(pred_bboxes[None], gt_bboxes[:, None], xywh=False).squeeze(0)  # (n_gt, na)\n",
        "\n",
        "    # Применяем маску\n",
        "    bbox_scores = bbox_scores * mask_in_gts.float()\n",
        "    overlaps = overlaps * mask_in_gts.float()\n",
        "\n",
        "    # TAL метрика\n",
        "    align_metric = bbox_scores.pow(6.0) * overlaps.pow(1.0)\n",
        "    return align_metric, overlaps  # (n_gt, na)\n",
        "\n",
        "\n",
        "def _select_topk_candidates(align_metric, topk):\n",
        "    \"\"\"Top-k по метрике для каждого GT.\"\"\"\n",
        "    topk_metrics, topk_idxs = torch.topk(align_metric, topk, dim=-1)  # (n_gt, topk)\n",
        "\n",
        "    # Маска ненулевых метрик\n",
        "    topk_mask = topk_metrics.max(-1, keepdim=True)[0] > 1e-9\n",
        "    topk_idxs = torch.where(topk_mask, topk_idxs, 0)\n",
        "\n",
        "    # Уникальные якоря (без дубликатов)\n",
        "    count_tensor = torch.zeros_like(align_metric)\n",
        "    for k in range(topk):\n",
        "        count_tensor.scatter_add_(-1, topk_idxs[:, k:k+1], 1)\n",
        "    count_tensor = (count_tensor == 1).float()\n",
        "\n",
        "    return count_tensor  # (n_gt, na)\n",
        "\n",
        "\n",
        "def _select_highest_overlaps(mask_pos, overlaps, n_gt):\n",
        "    \"\"\"Один якорь -> один GT (max IoU).\"\"\"\n",
        "    fg_mask = mask_pos.sum(0)  # (na,)\n",
        "\n",
        "    if fg_mask.max() > 1:  # Конфликты\n",
        "        max_overlaps_idx = overlaps.argmax(0, keepdim=True)  # (1, na)\n",
        "        is_max_overlaps = torch.zeros_like(mask_pos)\n",
        "        is_max_overlaps.scatter_(0, max_overlaps_idx, 1)\n",
        "        mask_pos = is_max_overlaps * mask_pos\n",
        "        fg_mask = mask_pos.sum(0)\n",
        "\n",
        "    target_gt_idx = mask_pos.argmax(0)  # (na,)\n",
        "    return target_gt_idx, fg_mask, mask_pos\n",
        "\n",
        "\n",
        "def _get_targets(gt_labels, gt_bboxes, target_gt_idx, fg_mask):\n",
        "    \"\"\"Формирует targets.\"\"\"\n",
        "    target_labels = torch.full((len(fg_mask),), -1, device=gt_labels.device)  # bg=-1\n",
        "    target_bboxes = torch.zeros((len(fg_mask), 4), device=gt_bboxes.device)\n",
        "\n",
        "    fg_inds = fg_mask.nonzero().squeeze(-1)\n",
        "    if len(fg_inds) > 0:\n",
        "        target_labels[fg_inds] = gt_labels[target_gt_idx[fg_inds]]\n",
        "        target_bboxes[fg_inds] = gt_bboxes[target_gt_idx[fg_inds]]\n",
        "\n",
        "    # One-hot scores\n",
        "    target_scores = torch.zeros((len(fg_mask), gt_labels.max().item() + 1), device=gt_labels.device)\n",
        "    fg_labels = target_labels[fg_mask]\n",
        "    target_scores[fg_mask.nonzero().squeeze(-1), fg_labels.long()] = 1.0\n",
        "\n",
        "    return target_labels, target_bboxes, target_scores\n",
        "\n",
        "\n",
        "def _normalize_target_scores(target_scores, align_metric, overlaps, mask_pos):\n",
        "    \"\"\"Нормализация по TAL метрике.\"\"\"\n",
        "    align_metric = align_metric * mask_pos.float()\n",
        "    pos_align_metrics = align_metric.max(0, keepdim=True)[0]  # (1, na)\n",
        "    pos_overlaps = (overlaps * mask_pos.float()).max(0, keepdim=True)[0]  # (1, na)\n",
        "\n",
        "    norm_align_metric = (align_metric * pos_overlaps / (pos_align_metrics + 1e-9)).max(0).unsqueeze(-1)\n",
        "    return target_scores * norm_align_metric"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28f25b75-58ed-44e7-adca-c5d518a7467b",
      "metadata": {
        "id": "28f25b75-58ed-44e7-adca-c5d518a7467b"
      },
      "source": [
        "### DIoU [1]\n",
        "\n",
        "Вместо SmoothL1, который используется в семинаре, реализуем лосс, основанный на пересечении ббоксов. В качестве тренировки давайте напишем Distance Intersection over Union (DIoU).\n",
        "\n",
        "<center><img src=https://wikidocs.net/images/page/163613/Free_Fig_5.png></center>\n",
        "\n",
        "Для его реализации разобъем задачу на части:\n",
        "\n",
        "**1. Реализуем IoU:**\n",
        "\n",
        "Пусть даны координаты для предсказанного ($B^p$) и истинного ($B^g$) ббоксов в формате XYXY или VOC PASCAL (левый верхний и правый нижний углы):\n",
        "\n",
        "$B^p=(x^p_1, y^p_1, x^p_2, y^p_2)$, $B^g=(x^g_1, y^g_1, x^g_2, y^g_2)$, тогда алгоритм расчета будет следующий:\n",
        "\n",
        "    1. Найдем площади обоих ббоксов:\n",
        "$$ A^p = (x^p_2 - x^p_1) * (y^p_2 - y^p_1) $$\n",
        "$$ A^g = (x^g_2 - x^g_1) * (y^g_2 - y^g_1) $$\n",
        "\n",
        "    2. Посчитаем пересечение между ббоксами:\n",
        "\n",
        "Тут мы предлагаем вам подумать как в общем виде можно расчитать размеры ббокса, который будет являться пересечением $B^p$ и $B^g$, а затем посчитать его площадь:\n",
        "\n",
        "$$x^I_1 = \\qquad \\qquad y^I_1 = $$\n",
        "$$x^I_2 = \\qquad \\qquad y^I_2 = $$\n",
        "\n",
        "В общем виде, площать будет записываться следующим образом:\n",
        "\n",
        "Если $x^I_2 > x^I_1$ & $y^I_2 > y^I_1$, тогда:\n",
        "\n",
        "$$I = (x^I_2 - x^I_1) * (y^I_2 - y^I_1)$$\n",
        "\n",
        "Иначе, $I = 0$.\n",
        "\n",
        "    3. Считаем объединение ббоксов.\n",
        "\n",
        "Мы можем посчитать эту площадь как сумму площадей двух ббоксов минус площадь пересечения (тк мы считаем её два раз в сумме площадей):\n",
        "\n",
        "$$U = A^p + A^g - I$$\n",
        "\n",
        "    4. Вычисляем IoU.\n",
        "\n",
        "$$IoU = \\frac{I}{U}$$\n",
        "\n",
        "**2. Посчитаем диагональ выпуклой оболочки:**\n",
        "\n",
        "Для расчета диагонали, сначала выпишите координаты верхнего левого и правого нижнего углов. Подумайте, чему будут равны эти координаты в общем случае?\n",
        "\n",
        "$$x^c_1 = \\qquad \\qquad y^c_1 = $$\n",
        "$$x^c_2 = \\qquad \\qquad y^c_2 = $$\n",
        "\n",
        "Подсказка: Нарисуйте несколько вариантов пересечений предсказания и GT на бумажке, и выпишите координаты для выпуклой оболочки.\n",
        "\n",
        "Тогда квадрат диагонали можно посчитать по формуле:\n",
        "\n",
        "$$c^2 = (x^c_2 - x^c_1)^2 + (y^c_2 - y^c_1)^2$$\n",
        "\n",
        "**3. Рассчитаем расстояние между цетрами ббоксов:**\n",
        "\n",
        "Сначала находим координаты центров каждого из ббоксов (если ббоксы в формате YOLO, то и считать ничего не нужно), затем считаем Евклидово расстояние между центрами.\n",
        "\n",
        "$d = $\n",
        "\n",
        "Собираем все части вместе и считаем лосс по формуле:\n",
        "\n",
        "$$ DIoU = 1 - IoU + \\frac{d^2}{c^2}$$\n",
        "\n",
        "Помните, что пар ббоксов может быть много! Возвращайте усредненное значение лосса."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c3740d0-76f7-43b6-a4bd-5ab7048d0c70",
      "metadata": {
        "id": "0c3740d0-76f7-43b6-a4bd-5ab7048d0c70"
      },
      "outputs": [],
      "source": [
        "from torchvision.ops import distance_box_iou_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c412008-e90e-4087-9725-6f4777dc3c8b",
      "metadata": {
        "id": "1c412008-e90e-4087-9725-6f4777dc3c8b"
      },
      "outputs": [],
      "source": [
        "def gen_bbox(num_boxes=10):\n",
        "    min_corner = torch.randint(0, 100, (num_boxes, 2))\n",
        "    max_corner = torch.randint(50, 150, (num_boxes, 2))\n",
        "\n",
        "    for i in range(2):\n",
        "        wrong_order = min_corner[:, i] > max_corner[:, i]\n",
        "        if wrong_order.any():\n",
        "            min_corner[wrong_order, i], max_corner[wrong_order, i] = max_corner[wrong_order, i], min_corner[wrong_order, i]\n",
        "    return torch.cat((min_corner, max_corner), dim=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c764214-2797-4111-8e5f-d421dad9aae6",
      "metadata": {
        "id": "6c764214-2797-4111-8e5f-d421dad9aae6"
      },
      "outputs": [],
      "source": [
        "pred_boxes = gen_bbox(num_boxes=100)\n",
        "true_boxes = gen_bbox(num_boxes=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24a64210-29a0-4b1a-a0d1-c690b1955801",
      "metadata": {
        "id": "24a64210-29a0-4b1a-a0d1-c690b1955801"
      },
      "outputs": [],
      "source": [
        "print(f\" DIoU: {distance_box_iou_loss(pred_boxes, true_boxes, reduction=\"mean\").item()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2dd590f7-59fc-4aae-93cd-23dc01d8cf3d",
      "metadata": {
        "id": "2dd590f7-59fc-4aae-93cd-23dc01d8cf3d"
      },
      "outputs": [],
      "source": [
        "def diou_loss(pred_boxes, gt_boxes, eps=1e-6):\n",
        "\n",
        "    px1, py1, px2, py2 = pred_boxes.unbind(dim=1)\n",
        "    gx1, gy1, gx2, gy2 = gt_boxes.unbind(dim=1)\n",
        "\n",
        "    area_p = (px2 - px1) * (py2 - py1)\n",
        "    area_g = (gx2 - gx1) * (gy2 - gy1)\n",
        "\n",
        "    ix1 = torch.max(px1, gx1)\n",
        "    iy1 = torch.max(py1, gy1)\n",
        "    ix2 = torch.min(px2, gx2)\n",
        "    iy2 = torch.min(py2, gy2)\n",
        "\n",
        "    inter_w = (ix2 - ix1).clamp(min=0)\n",
        "    inter_h = (iy2 - iy1).clamp(min=0)\n",
        "    inter = inter_w * inter_h\n",
        "\n",
        "\n",
        "    union = area_p + area_g - inter + eps\n",
        "\n",
        "    iou = inter / union\n",
        "\n",
        "    cx1 = torch.min(px1, gx1)\n",
        "    cy1 = torch.min(py1, gy1)\n",
        "    cx2 = torch.max(px2, gx2)\n",
        "    cy2 = torch.max(py2, gy2)\n",
        "\n",
        "    c2 = (cx2 - cx1)**2 + (cy2 - cy1)**2 + eps\n",
        "\n",
        "    pcx = (px1 + px2) / 2\n",
        "    pcy = (py1 + py2) / 2\n",
        "\n",
        "    gcx = (gx1 + gx2) / 2\n",
        "    gcy = (gy1 + gy2) / 2\n",
        "\n",
        "    d2 = (pcx - gcx)**2 + (pcy - gcy)**2\n",
        "\n",
        "    diou = 1 - iou + d2 / c2\n",
        "\n",
        "    return diou.mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "169ebd6b-c45f-4b7d-a456-ff3e95619136",
      "metadata": {
        "id": "169ebd6b-c45f-4b7d-a456-ff3e95619136"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "pred_boxes = gen_bbox(num_boxes=1000)\n",
        "true_boxes = gen_bbox(num_boxes=1000)\n",
        "\n",
        "# проверим что написанный лосс выдает те же результаты что и лосс из торча.\n",
        "assert np.isclose(diou_loss(pred_boxes, true_boxes), distance_box_iou_loss(pred_boxes, true_boxes, reduction=\"mean\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1167d47f-9a56-40e2-9bb2-f8e749c5e499",
      "metadata": {
        "id": "1167d47f-9a56-40e2-9bb2-f8e749c5e499"
      },
      "source": [
        "## Кто больше? [5 баллов]\n",
        "\n",
        "Наконец то мы дошли до самый интересной части. Тут мы раздаем очки за mAP'ы!\n",
        "\n",
        "Все что вы написали выше вам поможет улучшить качество итогового детектора, настало время узнать насколько сильно :)\n",
        "\n",
        "За достижения порога по mAP на тестовом наборе вы получаете баллы:\n",
        "* 0.05 mAP [1]\n",
        "* 0.1 mAP [2]\n",
        "* 0.2 mAP [5]\n",
        "\n",
        "\n",
        "**TIPS**:\n",
        "1. На семинаре мы специально не унифицировали формат ббоксов между методами, чтобы обратить ваше внимание что за этим нужно следить. Чтобы было проще, сразу унифицируете формат по всему ноутбуку. Советуем использовать формат xyxy, тк IoU и NMS из torch используют именно этот формат. (Не забудьте поменять формат у таргета в `HaloDataset`).\n",
        "\n",
        "2. Попробуйте перейти к IoU-based лоссу при обучении. То есть обучать не смещения, а сразу предсказывать ббокс.\n",
        "\n",
        "3. Поэксперементируйте с подходами target assignment'а в процессе обучения. Например, можно на первых итерациях использовать обычный метод, а затем подключить TAL.\n",
        "\n",
        "4. Добавьте аугментаций!\n",
        "\n",
        "Можно взять [albumentations](https://albumentations.ai/docs/getting_started/bounding_boxes_augmentation/), библиотеку, которую мы использовали всеминаре. Или базовые аугментации из торча [тык](https://pytorch.org/vision/main/transforms.html). Если будете использовать торч, не забудте про ббоксы, transforms из коробки не будет их агументировать.\n",
        "\n",
        "5. Можете реализовать другую шею, которую мы обсуждали на лекции [Path Aggregation Network](https://arxiv.org/abs/1803.01534) она точно улучшит ваше итоговое качество.\n",
        "\n",
        "6. Попробуйте добавлять различные блоки из YOLO архитектур в шею вместо единичных конволюционных слоев. (Например, замените конволюции 3х3 на CSP блоки).\n",
        "\n",
        "7. Попробуйте заменить NMS на другой метод (WeightedNMS, SoftNMS, etc.). Немного ссылок:\n",
        "    * Статья про SoftNMS [тык](https://arxiv.org/pdf/1704.04503)\n",
        "    * Статья про WeightedNMS [тык](https://openaccess.thecvf.com/content_ICCV_2017_workshops/papers/w14/Zhou_CAD_Scale_Invariant_ICCV_2017_paper.pdf)\n",
        "    * Есть их реализация, правда на нумбе [git](https://github.com/ZFTurbo/Weighted-Boxes-Fusion?tab=readme-ov-file)\n",
        "\n",
        "8. Не бойтесь эксперементировать и удачи!\n",
        "\n",
        "Также, напишите развернутые ответы на следующие вопросы:\n",
        "\n",
        "**Questions:**\n",
        "1. Какой метод label assignment'a помогает лучше обучаться модели? Почему?\n",
        "2. Какое из сделаных вами улучшений внесло наибольший вклад в качество модели? Как вы думаете, почему это произошло?\n",
        "3. Какое из сделанных вами улучшений вообще не изменило метрику? Как вы думаете, почему это произошло?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ea5c0ec-c738-4c11-b6ec-dd1bf9a9491d",
      "metadata": {
        "id": "6ea5c0ec-c738-4c11-b6ec-dd1bf9a9491d"
      },
      "source": [
        "Ниже определена вспомогательная функция для валидации качества. Можете использовать `Runner.validate`. Важное уточнение, ей нужен метод для фильтрации предсказаний. Можете тоже скопировать его из семинара, если он у вас не менялся."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pycocotools.coco import COCO\n",
        "from pycocotools.cocoeval import COCOeval\n",
        "\n",
        "@torch.no_grad()\n",
        "def compute_coco_map(model, dataloader, device, score_threshold=0.01, nms_threshold=0.5, num_classes=4):\n",
        "    \"\"\" Считаем mAP модели на данных из `dataloader`. \"\"\"\n",
        "    model.eval()\n",
        "    ann_id = 1\n",
        "    all_detections = []\n",
        "    all_gt_annotations = []\n",
        "    images_info = []\n",
        "\n",
        "    for images, targets in tqdm(dataloader, desc=\"Dataset Evaluation\"):\n",
        "        # Делаем предсказание для всех картинок в батче\n",
        "        predictions = predict(model, images, device, score_threshold, nms_threshold)\n",
        "        # Сохраняем изначальные картинки, предсказания и таргет в формате COCO\n",
        "        for i in range(images.shape[0]):\n",
        "            image_id = targets[i][\"image_id\"]\n",
        "            images_info.append({\n",
        "                \"id\": image_id,\n",
        "                \"width\": images[i].shape[1],\n",
        "                \"height\": images[i].shape[2]\n",
        "            })\n",
        "\n",
        "            # Сохраняем предсказания модели в формате COCO\n",
        "            img_pred = predictions[i]\n",
        "            for box, cls, sc in zip(img_pred[\"boxes\"], img_pred[\"labels\"], img_pred[\"scores\"]):\n",
        "                all_detections.append({\n",
        "                    \"image_id\": int(targets[i][\"image_id\"]),  # int() сразу\n",
        "                    \"category_id\": int(cls) + 1,      # int()\n",
        "                    \"bbox\": [float(x) for x in box], # float()\n",
        "                    \"score\": float(sc)                # float()\n",
        "                })\n",
        "\n",
        "            # Сохраняем таргет в формате COCO\n",
        "            gt_boxes = targets[i]['boxes'].cpu().numpy().tolist()\n",
        "            gt_labels = targets[i]['labels'].cpu().numpy().tolist()\n",
        "            for box, label in zip(gt_boxes, gt_labels):\n",
        "                gt_annotation = {\n",
        "                    \"id\": int(ann_id),\n",
        "                    \"image_id\": int(image_id),\n",
        "                    \"category_id\": int(label) + 1,\n",
        "                    \"bbox\": [float(x) for x in box],\n",
        "                    \"area\": float(box[2] * box[3]),\n",
        "                    \"iscrowd\": 0\n",
        "                }\n",
        "                all_gt_annotations.append(gt_annotation)\n",
        "                ann_id += 1\n",
        "\n",
        "    coco_gt_dict = {\n",
        "        \"info\": {},\n",
        "        \"images\": images_info,\n",
        "        \"annotations\": all_gt_annotations,\n",
        "        \"categories\": [{\"id\": i+1, \"name\": f\"class_{i}\"} for i in range(model.num_classes)]\n",
        "    }\n",
        "\n",
        "    coco_gt = COCO()\n",
        "\n",
        "    coco_gt.dataset = coco_gt_dict\n",
        "\n",
        "    coco_gt.createIndex()\n",
        "    import json, tempfile\n",
        "    with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False) as f:\n",
        "        json.dump(all_detections, f)\n",
        "        det_file = f.name\n",
        "    coco_dt = coco_gt.loadRes(det_file)\n",
        "\n",
        "    coco_eval = COCOeval(coco_gt, coco_dt, iouType='bbox')\n",
        "    coco_eval.evaluate()\n",
        "    coco_eval.accumulate()\n",
        "    coco_eval.summarize()\n",
        "\n",
        "    overall_mAP = coco_eval.stats[0]\n",
        "    print(f\"Validation mAP: {overall_mAP:.4f}\\n\\n\")\n",
        "\n",
        "    class_maps = {}\n",
        "    for cat_id in range(1, num_classes + 1):\n",
        "        class_name = class_to_name[cat_id]\n",
        "        print(f\"\\nmAP for class {class_name}\")\n",
        "        print(\"-\" * 50)\n",
        "        coco_eval_cat = COCOeval(coco_gt, coco_dt, iouType='bbox')\n",
        "        coco_eval_cat.params.catIds = [cat_id]\n",
        "        coco_eval_cat.params.imgIds = coco_gt.getImgIds(catIds=[cat_id])\n",
        "        coco_eval_cat.evaluate()\n",
        "        coco_eval_cat.accumulate()\n",
        "        coco_eval_cat.summarize();\n",
        "        ap = coco_eval_cat.stats[0]\n",
        "        class_maps[cat_id] = ap"
      ],
      "metadata": {
        "id": "dTcey_kfsFof"
      },
      "id": "dTcey_kfsFof",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score_threshold = 0.1\n",
        "nms_threshold = 0.5\n",
        "compute_coco_map(model, test_dataloader, device=device, score_threshold=score_threshold, nms_threshold=nms_threshold)"
      ],
      "metadata": {
        "id": "ZJGVldsZsHkC"
      },
      "id": "ZJGVldsZsHkC",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "158f2ea4fe3340e49afdd9d3475c43bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_95868ce8350d48aba18da8d072d291d0",
              "IPY_MODEL_0c8cf014acda4e169082c520aec0ce97",
              "IPY_MODEL_f02110be3bcb4a579062a623c0affe56"
            ],
            "layout": "IPY_MODEL_2c2b771d29ee423288a88d5cbf2be9e9"
          }
        },
        "95868ce8350d48aba18da8d072d291d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1c8a8094b6a74122a2d50b3eefa60b93",
            "placeholder": "​",
            "style": "IPY_MODEL_d356825c65e04a4493fb63d9a7950924",
            "value": "model.safetensors: 100%"
          }
        },
        "0c8cf014acda4e169082c520aec0ce97": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_edf9cb94ae2c4f3697019da5b690dd99",
            "max": 21355344,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b96949a7fff64050a5c1ec5cc008424e",
            "value": 21355344
          }
        },
        "f02110be3bcb4a579062a623c0affe56": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c1207abc62fd473eb3cbb68a7767d85e",
            "placeholder": "​",
            "style": "IPY_MODEL_05e418e1d7fa45fc82c28bc2a6ee4af0",
            "value": " 21.4M/21.4M [00:02&lt;00:00, 12.3MB/s]"
          }
        },
        "2c2b771d29ee423288a88d5cbf2be9e9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1c8a8094b6a74122a2d50b3eefa60b93": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d356825c65e04a4493fb63d9a7950924": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "edf9cb94ae2c4f3697019da5b690dd99": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b96949a7fff64050a5c1ec5cc008424e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c1207abc62fd473eb3cbb68a7767d85e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "05e418e1d7fa45fc82c28bc2a6ee4af0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e2d2c43a7ac24dedbcb6d8329e54eb6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_59cdb944fdb94ec78444b90cb45f7aed",
              "IPY_MODEL_3b45fa25c265461c86eaf94dd3357f0c",
              "IPY_MODEL_6e6a81e5d7704cfba413c13df93f4ce1"
            ],
            "layout": "IPY_MODEL_1168d40bb8d84aab9f9d59c6bc806b21"
          }
        },
        "59cdb944fdb94ec78444b90cb45f7aed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a17025e0c8e445d48e38e4ef7d49f6ef",
            "placeholder": "​",
            "style": "IPY_MODEL_3f2f9afdc19c4f58b9f3b871e05abc14",
            "value": "Train epoch:   0%"
          }
        },
        "3b45fa25c265461c86eaf94dd3357f0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_39686ce4303944a89bc8e6b7006cb17f",
            "max": 100,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_896a152fc4af461d9458cf2ca4b504d2",
            "value": 0
          }
        },
        "6e6a81e5d7704cfba413c13df93f4ce1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_40954eebb58e4e08ae6a51246fda6e2f",
            "placeholder": "​",
            "style": "IPY_MODEL_1921992aca6d4abaa2d8c8983de12f76",
            "value": " 0/100 [00:10&lt;?, ?it/s]"
          }
        },
        "1168d40bb8d84aab9f9d59c6bc806b21": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a17025e0c8e445d48e38e4ef7d49f6ef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3f2f9afdc19c4f58b9f3b871e05abc14": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "39686ce4303944a89bc8e6b7006cb17f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "896a152fc4af461d9458cf2ca4b504d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "40954eebb58e4e08ae6a51246fda6e2f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1921992aca6d4abaa2d8c8983de12f76": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "02bc60d1620147f69614ee4f10cdc565": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_452bda554ebf46ea81e0e09bda05adac",
              "IPY_MODEL_45c34ab41be9440fa539026f0b284774",
              "IPY_MODEL_0f52dc57dd4d44c19f3d652c4da8654f"
            ],
            "layout": "IPY_MODEL_71962b499338466daace74a202affe35"
          }
        },
        "452bda554ebf46ea81e0e09bda05adac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6953d937b05944419ab7d79422d56172",
            "placeholder": "​",
            "style": "IPY_MODEL_eb833deab8e146959c23046d2df221a6",
            "value": "Process train epoch:   0%"
          }
        },
        "45c34ab41be9440fa539026f0b284774": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_82b019971a5e4aebb60818aaac66e9ae",
            "max": 8,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bec7f6596ab644eaac68a17304112ef8",
            "value": 0
          }
        },
        "0f52dc57dd4d44c19f3d652c4da8654f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e1740bc35ff2404d89dc8b2ac0fdb54a",
            "placeholder": "​",
            "style": "IPY_MODEL_5b1d7bbdc3c24a97ad716338d9842ac0",
            "value": " 0/8 [00:10&lt;?, ?it/s]"
          }
        },
        "71962b499338466daace74a202affe35": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6953d937b05944419ab7d79422d56172": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eb833deab8e146959c23046d2df221a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "82b019971a5e4aebb60818aaac66e9ae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bec7f6596ab644eaac68a17304112ef8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e1740bc35ff2404d89dc8b2ac0fdb54a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5b1d7bbdc3c24a97ad716338d9842ac0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}